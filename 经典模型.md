# 经典模型
本部分收集自transformers以来经典的预训练模型（NLP 领域）文章

## 目录

- [经典模型](#经典模型)
  - [目录](#目录)
  - [概念解释，什么是预训练模型，什么是foundation model?](#概念解释什么是预训练模型什么是foundation-model)
  - [发展概览](#发展概览)
  - [文章列表](#文章列表)
    - [综述](#综述)
    - [中小规模的经典模型](#中小规模的经典模型)
    - [Foudation model](#foudation-model)
  - [其它参考资料](#其它参考资料)

## 概念解释，什么是预训练模型，什么是foundation model?

## 发展概览
![发展历程](image/发展历程.jpg)
*注，上图取自：https://github.com/Mooler0410/LLMsPracticalGuide*

## 文章列表
### 综述
- On the Opportunities and Risk of Foundation Models ,Preprint 2022.
  
  **\*Rishi Bommasani\* Drew A. Hudson Ehsan Adeli Russ Altman Simran Arora Sydney von Arx Michael S. Bernstein Jeannette Bohg Antoine Bosselut Emma Brunskill Erik Brynjolfsson Shyamal Buch Dallas Card Rodrigo Castellon Niladri Chatterji Annie Chen Kathleen Creel Jared Quincy Davis Dorottya Demszky Chris Donahue Moussa Doumbouya Esin Durmus Stefano Ermon John Etchemendy Kawin Ethayarajh Li Fei-Fei**
  [[pdf](https://arxiv.org/pdf/2108.07258.pdf)]

- A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT, Preprint 2023.
  
  **Ce Zhou1\* Qian Li2\∗ Chen Li2∗ Jun Yu3∗ Yixin Liu3∗ Guangjing Wang1 Kai Zhang3 Cheng Ji2 Qiben Yan1 Lifang He3 Hao Peng2 Jianxin Li2 Jia Wu4 Ziwei Liu5 Pengtao Xie6 Caiming Xiong7 Jian Pei8 Philip S. Yu9 Lichao Sun3\***
  [[pdf](https://arxiv.org/pdf/2302.09419.pdf)]



### 中小规模的经典模型
- Transformers: Attention is all you need, Preprint 2017. [[pdf](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)]
  
  *注：严格来说，transformers不算是预训练模型，它是一种架构，且被提出的目的是用于机器翻译方向。*

- GPT: Improving Language Understanding by Generative Pre-Training, Preprint 2018. [[pdf](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)]

- BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, Preprint 2019. [[pdf](https://arxiv.org/pdf/1810.04805.pdf)]

- T5: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer, Preprint xx. [[pdf](https://arxiv.org/pdf/1910.10683.pdf)]

- Longformer: The Long-Document Transformer, Preprint 2020. [[pdf](https://arxiv.org/pdf/2004.05150.pdf?forcedefault=true)]


### Foudation model
- instructGPT: Training language models to follow instructions with human feedbac, Preprint 2022. [[pdf](https://arxiv.org/pdf/2203.02155.pdf)]

- webGPT: Browser-assisted question-answering with human feedback, Preprint 2022. [[pdf](https://arxiv.org/pdf/2112.09332.pdf)]

- GLM: AN OPEN BILINGUAL PRE-TRAINEDMODEL, Preprint 2022. [[pdf](https://arxiv.org/pdf/2210.02414.pdf)]

- llama:  Open and Efficient Foundation Language Models, Preprint 2023. [[pdf](https://arxiv.org/pdf/2302.13971v1.pdf)]

- Alpaca: A Strong, Replicable Instruction-Following Model, Preprint 2023. [[pdf](https://crfm.stanford.edu/2023/03/13/alpaca.html)]


- GPT4技术报告: GPT-4 Technical Report, Preprint 2023. [[pdf](https://cdn.openai.com/papers/gpt-4.pdf)]

## 其它参考资料
- 对预训练模型的分类介绍：https://zhuanlan.zhihu.com/p/401414083