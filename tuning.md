
# tuning
本部分收集各类tuning方法的相关文章

## 目录
-[概念解释](#概念解释)

-[文章列表](#文章列表)

## 概念解释
**什么是fine-tuning, peft, instruction tuning**

## 文章列表

### 综述
- delta-tuning: A Comprehensive Study of Parameter Efficient Methods for Pre-trained Language Models, Preprint 2022. [[pdf](https://arxiv.org/pdf/2203.06904.pdf)]

### peft
- Adapter-tuning: Parameter-Efficient Transfer Learning for NLP, Preprint 2019. [[pdf](https://arxiv.org/pdf/1902.00751.pdf)]

- prompt-tuning: The Power of Scale for Parameter-Efficient Prompt Tuning, Preprint 2021. [[pdf](https://arxiv.org/pdf/2203.06904.pdf)]

- prefix-tuning: Optimizing Continuous Prompts for Generation, Preprint 2021. [[pdf](https://arxiv.org/pdf/2101.00190.pdf)]

- p-tuning: GPT Understands, Too, Preprint 2021. [[pdf](https://arxiv.org/pdf/2103.10385.pdf)]

- LoRa LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS , Preprint 2021. [[pdf](https://arxiv.org/pdf/2106.09685.pdf)]

- p-tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks , Preprint 2022. [[pdf](https://arxiv.org/pdf/2110.07602.pdf)]

- CoT:Chain-of-Thought Prompting Elicits Reasoning in Large Language Models, Preprint 2023. [[pdf](https://arxiv.org/pdf/2201.11903.pdf)]
  
  *注：CoT有一系列文章，可参考：https://github.com/Timothyxxx/Chain-of-ThoughtsPapers*

- ToT:Tree of Thoughts: Deliberate Problem Solving with Large Language Models, Preprint 2023. [[pdf](https://arxiv.org/pdf/2305.10601.pdf)]

- QLORA: Efficient Finetuning of Quantized LLMs , Preprint 2023. [[pdf](https://arxiv.org/pdf/2305.14314.pdf)]


### instruction-tuning
- FINETUNED LANGUAGE MODELS ARE ZERO-SHOT LEARNERS, Preprint 2022.1.8. [[pdf](https://arxiv.org/pdf/2305.14314.pdf)]
  
  *instruction tuning的首次提出*

- Unnatural Instructions: Tuning Language Models with (Almost) No Human Labor, Preprint 2022 [[pdf](https://arxiv.org/pdf/2212.09689.pdf)]

- RLHF: Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback, Preprint 2022 [[pdf](https://arxiv.org/pdf/2204.05862.pdf)]


## 其他资料
- 让天下没有难Tuning的大模型-PEFT技术简介 https://www.yuque.com/meta95/hmc3l4/ozgy13dx4akv7v17?singleDoc#miOqk

- Prompt-Tuning——深度解读一种新的微调范式 - 华师数据王嘉宁的文章 - 知乎
https://zhuanlan.zhihu.com/p/619566088


- **什么是incontext-learning?**
https://arxiv.org/pdf/2301.00234.pdf
